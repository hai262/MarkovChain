{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b265e907-fabe-44ed-b2b9-30183c78f155",
   "metadata": {},
   "source": [
    "Project 1: Text Complexity and Readability Evaluation of Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ab63d-ed34-4497-8d09-6eaf91425662",
   "metadata": {},
   "source": [
    "Objective: Evaluate the readability of text generated by the Markov Chain and compare it to that of human-generated text. \n",
    "Process\r\n",
    "Use Markov Chains to generate text based on different training data set.\r\n",
    "Calculate readability scores (e.g., Flesch-Kincaid, Gunning Fog Index) for both generated and real tex s.\r\n",
    "Compare readability across text samples to see if your generator can mimic the complexity of real-world t xt.\r\n",
    "Evaluaion:\r\n",
    "Plot readability scores and analyze them statisti ally.\r\n",
    "Assess the variance in complexity between generated and original texts for diverse data s  ources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b219459c-2f13-4541-a601-edf1b97b7d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MarkovChain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MarkovChain.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import textstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from scipy.stats import ttest_ind\n",
    "from io import StringIO\n",
    "\n",
    "# --- Page Configuration ---\n",
    "st.set_page_config(page_title=\"Text Complexity & Readability Evaluation\", layout=\"wide\")\n",
    "\n",
    "# --- Custom CSS for styling ---\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .stApp {\n",
    "        background-color: #f5f5f5;\n",
    "    }\n",
    "    /* Custom button colors using CSS selectors on the horizontal block */\n",
    "    div[data-testid=\"stHorizontalBlock\"] > div:nth-child(1) button {\n",
    "      background-color: #ff4b4b;\n",
    "      color: white;\n",
    "    }\n",
    "    div[data-testid=\"stHorizontalBlock\"] > div:nth-child(2) button {\n",
    "      background-color: #4bff4b;\n",
    "      color: white;\n",
    "    }\n",
    "    div[data-testid=\"stHorizontalBlock\"] > div:nth-child(3) button {\n",
    "      background-color: #4b4bff;\n",
    "      color: white;\n",
    "    }\n",
    "    div[data-testid=\"stHorizontalBlock\"] > div:nth-child(4) button {\n",
    "      background-color: #ffa500;\n",
    "      color: white;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Sidebar Options ---\n",
    "st.sidebar.header(\"Configuration Options\")\n",
    "input_method = st.sidebar.radio(\"Select Input Method\", (\"Upload Text File\", \"Paste Text\"))\n",
    "\n",
    "if input_method == \"Upload Text File\":\n",
    "    uploaded_file = st.sidebar.file_uploader(\"Upload a .txt file\", type=[\"txt\"])\n",
    "    if uploaded_file:\n",
    "        raw_text = uploaded_file.read().decode(\"utf-8\")\n",
    "    else:\n",
    "        raw_text = None\n",
    "else:\n",
    "    raw_text = st.sidebar.text_area(\"Paste your text here\", height=200)\n",
    "\n",
    "# --- Control Cover Page Display ---\n",
    "if \"data_uploaded\" not in st.session_state:\n",
    "    st.session_state[\"data_uploaded\"] = False\n",
    "\n",
    "if raw_text:\n",
    "    st.session_state[\"data_uploaded\"] = True\n",
    "\n",
    "cover_image_url = \"https://fiverr-res.cloudinary.com/images/q_auto,f_auto/gigs/331947776/original/81fbd94368e4ecdeaa7d502528ab7cebb7a6df58/convert-ai-generated-text-into-authentic-human-writing.png\"\n",
    "\n",
    "# Display cover page only when no data is uploaded\n",
    "if not st.session_state[\"data_uploaded\"]:\n",
    "    cover_html = f\"\"\"\n",
    "    <div style=\"position: relative; text-align: left; color: white;\">\n",
    "      <img src=\"{cover_image_url}\" alt=\"Cover Image\" style=\"width: 100%; opacity: 0.8;\">\n",
    "      <div style=\"position: absolute; top: 10%; left: 5%; transform: none;\">\n",
    "        <h1 style=\"font-size: 3em; margin: 0; color: red;\">Text Complexity & Readability Evaluation</h1>\n",
    "        <p style=\"font-size: 1.5em; margin-top: 10px; max-width: 90%; line-height: 1.4; color: white;\">\n",
    "          This project evaluates the complexity and readability of text generated by a Markov Chain model versus human-generated text. \n",
    "          Adjust the n-gram order, number of generated words, and number of samples. Explore various readability metrics through interactive visualizations.\n",
    "        </p>\n",
    "      </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    st.markdown(cover_html, unsafe_allow_html=True)\n",
    "\n",
    "# --- Additional Configuration Options (visible once data is uploaded) ---\n",
    "ngram_order = st.sidebar.slider(\"Choose n-gram order\", min_value=2, max_value=4, value=2, step=1)\n",
    "num_words_to_generate = st.sidebar.slider(\"Words to generate\", 20, 200, 50, step=10)\n",
    "num_samples = st.sidebar.slider(\"Number of samples\", 3, 20, 5, step=1)\n",
    "show_advanced_metrics = st.sidebar.checkbox(\"Show Advanced Readability Metrics\", value=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def build_ngram_model(text, n):\n",
    "    \"\"\"Build an n-gram Markov Chain model.\"\"\"\n",
    "    text_clean = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    words = text_clean.lower().split()\n",
    "    model = {}\n",
    "    for i in range(len(words) - n):\n",
    "        key = tuple(words[i:i+n])\n",
    "        next_word = words[i+n]\n",
    "        model.setdefault(key, []).append(next_word)\n",
    "    return model\n",
    "\n",
    "def generate_text_from_model(model, start_tuple, num_words):\n",
    "    \"\"\"Generate text from the n-gram model.\"\"\"\n",
    "    current_tuple = start_tuple\n",
    "    output = list(current_tuple)\n",
    "    for _ in range(num_words - len(current_tuple)):\n",
    "        next_words = model.get(current_tuple, None)\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word = random.choice(next_words)\n",
    "        output.append(next_word)\n",
    "        current_tuple = tuple(output[-len(current_tuple):])\n",
    "    return \" \".join(output)\n",
    "\n",
    "def calculate_readability(text, advanced=False):\n",
    "    \"\"\"Calculate readability metrics (excluding SMOG).\"\"\"\n",
    "    scores = {\n",
    "        \"Flesch-Kincaid\": textstat.flesch_kincaid_grade(text),\n",
    "        \"Gunning Fog\": textstat.gunning_fog(text),\n",
    "        \"ARI\": textstat.automated_readability_index(text)\n",
    "    }\n",
    "    if advanced:\n",
    "        scores[\"Coleman-Liau\"] = textstat.coleman_liau_index(text)\n",
    "        scores[\"Linsear Write\"] = textstat.linsear_write_formula(text)\n",
    "        scores[\"Dale Chall\"] = textstat.dale_chall_readability_score(text)\n",
    "    return scores\n",
    "\n",
    "# --- Main Logic ---\n",
    "if raw_text:\n",
    "    st.subheader(\"Original Text Preview\")\n",
    "    st.text_area(\"Original Text (first 1000 characters)\", raw_text[:1000], height=200)\n",
    "\n",
    "    # --- Build Markov Model ---\n",
    "    markov_model = build_ngram_model(raw_text, ngram_order)\n",
    "    unique_keys = list(markov_model.keys())\n",
    "    if not unique_keys:\n",
    "        st.error(\"Not enough data to build the Markov model. Please upload a larger text.\")\n",
    "    else:\n",
    "        # --- Generate Machine-Generated Samples ---\n",
    "        machine_texts = [generate_text_from_model(markov_model, random.choice(unique_keys), num_words_to_generate) \n",
    "                         for _ in range(num_samples)]\n",
    "        \n",
    "        # --- Prepare Human Text Samples ---\n",
    "        lines = raw_text.splitlines()\n",
    "        if len(lines) < num_samples:\n",
    "            human_texts = [raw_text] * num_samples\n",
    "        else:\n",
    "            chunk_size = len(lines) // num_samples\n",
    "            human_texts = [\" \".join(lines[i*chunk_size:(i+1)*chunk_size]) for i in range(num_samples)]\n",
    "        \n",
    "        # --- Compute Readability Scores ---\n",
    "        human_scores = [calculate_readability(text, advanced=show_advanced_metrics) for text in human_texts]\n",
    "        machine_scores = [calculate_readability(text, advanced=show_advanced_metrics) for text in machine_texts]\n",
    "\n",
    "        # --- Combine Results into DataFrame ---\n",
    "        human_data = [{\"Type\": \"Human\", **scores} for scores in human_scores]\n",
    "        machine_data = [{\"Type\": \"Machine\", **scores} for scores in machine_scores]\n",
    "        df_scores = pd.DataFrame(human_data + machine_data)\n",
    "\n",
    "        # --- Prepare T-test results for later use ---\n",
    "        t_test_results = {}\n",
    "        for metric in [\"Flesch-Kincaid\", \"Gunning Fog\", \"ARI\"]:\n",
    "            human_vals = df_scores[df_scores[\"Type\"] == \"Human\"][metric]\n",
    "            machine_vals = df_scores[df_scores[\"Type\"] == \"Machine\"][metric]\n",
    "            t_stat, p_val = ttest_ind(human_vals, machine_vals, nan_policy=\"omit\")\n",
    "            t_test_results[metric] = {\"T-Statistic\": t_stat, \"P-Value\": p_val}\n",
    "\n",
    "        # --- Section Navigation Buttons with Different Colors ---\n",
    "        if \"current_section\" not in st.session_state:\n",
    "            st.session_state.current_section = \"Overview\"\n",
    "\n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        if col1.button(\"Overview\"):\n",
    "            st.session_state.current_section = \"Overview\"\n",
    "        if col2.button(\"Detailed Analysis\"):\n",
    "            st.session_state.current_section = \"Detailed Analysis\"\n",
    "        if col3.button(\"Advanced Visualizations\"):\n",
    "            st.session_state.current_section = \"Advanced Visualizations\"\n",
    "        if col4.button(\"Download Results\"):\n",
    "            st.session_state.current_section = \"Download Results\"\n",
    "\n",
    "        st.markdown(f\"### {st.session_state.current_section}\")\n",
    "\n",
    "        # --- Render Section Content Based on Selection ---\n",
    "        if st.session_state.current_section == \"Overview\":\n",
    "            st.markdown(\"#### Readability Scores Summary\")\n",
    "            st.dataframe(df_scores)\n",
    "            \n",
    "            st.markdown(\"#### Readability Comparison Chart\")\n",
    "            melted_df = df_scores.melt(id_vars=[\"Type\"], var_name=\"Metric\", value_name=\"Score\")\n",
    "            plt.figure(figsize=(10,6))\n",
    "            sns.barplot(data=melted_df, x=\"Metric\", y=\"Score\", hue=\"Type\", palette=\"viridis\")\n",
    "            plt.title(\"Readability Score Comparison\", color='red')\n",
    "            plt.xticks(rotation=30)\n",
    "            st.pyplot(plt)\n",
    "            \n",
    "            st.markdown(\"#### Sample Texts\")\n",
    "            st.markdown(\"**Human Text Sample:**\")\n",
    "            st.write(human_texts[0])\n",
    "            st.markdown(\"**Machine-Generated Text Sample:**\")\n",
    "            st.write(machine_texts[0])\n",
    "        \n",
    "        elif st.session_state.current_section == \"Detailed Analysis\":\n",
    "            st.markdown(\"#### Statistical Analysis (T-test)\")\n",
    "            t_test_df = pd.DataFrame.from_dict(t_test_results, orient=\"index\").reset_index()\n",
    "            t_test_df.rename(columns={\"index\": \"Metric\"}, inplace=True)\n",
    "            st.dataframe(t_test_df)\n",
    "            \n",
    "            st.markdown(\"#### Correlation Matrix\")\n",
    "            corr = df_scores.drop(\"Type\", axis=1).corr()\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
    "            plt.title(\"Correlation Matrix\", color='navy')\n",
    "            st.pyplot(plt)\n",
    "            \n",
    "            st.markdown(\"#### Boxplots of Readability Metrics\")\n",
    "            melted_df = df_scores.melt(id_vars=[\"Type\"], var_name=\"Metric\", value_name=\"Score\")\n",
    "            plt.figure(figsize=(10,6))\n",
    "            sns.boxplot(data=melted_df, x=\"Metric\", y=\"Score\", hue=\"Type\", palette=\"Set2\")\n",
    "            plt.title(\"Boxplot Comparison of Readability Metrics\", color='darkgreen')\n",
    "            st.pyplot(plt)\n",
    "        \n",
    "        elif st.session_state.current_section == \"Advanced Visualizations\":\n",
    "            st.markdown(\"#### Interactive Scatter Matrix\")\n",
    "            fig_scatter = px.scatter_matrix(\n",
    "                df_scores,\n",
    "                dimensions=df_scores.columns.drop(\"Type\"),\n",
    "                color=\"Type\",\n",
    "                title=\"Scatter Matrix of Readability Metrics\"\n",
    "            )\n",
    "            st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "            \n",
    "            st.markdown(\"#### Violin Plot for Selected Metric\")\n",
    "            metric_selected = st.selectbox(\"Select a metric for distribution\", [\"Flesch-Kincaid\", \"Gunning Fog\", \"ARI\"])\n",
    "            fig_violin = px.violin(\n",
    "                df_scores, x=\"Type\", y=metric_selected, box=True, points=\"all\", \n",
    "                title=f\"Distribution of {metric_selected} Scores\"\n",
    "            )\n",
    "            st.plotly_chart(fig_violin, use_container_width=True)\n",
    "            \n",
    "            st.markdown(\"#### Histogram & Density Plot\")\n",
    "            plt.figure(figsize=(10,6))\n",
    "            for t in df_scores[\"Type\"].unique():\n",
    "                subset = df_scores[df_scores[\"Type\"] == t]\n",
    "                sns.kdeplot(subset[metric_selected], label=t, shade=True)\n",
    "            plt.title(f\"Density Plot for {metric_selected}\", color='purple')\n",
    "            st.pyplot(plt)\n",
    "            \n",
    "            st.markdown(\"#### Radar Chart: Average Readability Scores\")\n",
    "            base_metrics = [\"Flesch-Kincaid\", \"Gunning Fog\", \"ARI\"]\n",
    "            if show_advanced_metrics:\n",
    "                all_metrics = base_metrics + [\"Coleman-Liau\", \"Linsear Write\", \"Dale Chall\"]\n",
    "            else:\n",
    "                all_metrics = base_metrics\n",
    "                \n",
    "            avg_human = df_scores[df_scores[\"Type\"]==\"Human\"][all_metrics].mean()\n",
    "            avg_machine = df_scores[df_scores[\"Type\"]==\"Machine\"][all_metrics].mean()\n",
    "            radar_df = pd.DataFrame({\n",
    "                \"Metric\": all_metrics,\n",
    "                \"Human\": [avg_human.get(m, np.nan) for m in all_metrics],\n",
    "                \"Machine\": [avg_machine.get(m, np.nan) for m in all_metrics]\n",
    "            })\n",
    "            categories = radar_df[\"Metric\"].tolist()\n",
    "            fig_radar = go.Figure()\n",
    "            fig_radar.add_trace(go.Scatterpolar(\n",
    "                r=radar_df[\"Human\"].tolist(),\n",
    "                theta=categories,\n",
    "                fill='toself',\n",
    "                name='Human'\n",
    "            ))\n",
    "            fig_radar.add_trace(go.Scatterpolar(\n",
    "                r=radar_df[\"Machine\"].tolist(),\n",
    "                theta=categories,\n",
    "                fill='toself',\n",
    "                name='Machine'\n",
    "            ))\n",
    "            fig_radar.update_layout(\n",
    "                polar=dict(\n",
    "                    radialaxis=dict(\n",
    "                        visible=True,\n",
    "                        range=[min(radar_df[[\"Human\", \"Machine\"]].min()), max(radar_df[[\"Human\", \"Machine\"]].max())]\n",
    "                    )\n",
    "                ),\n",
    "                showlegend=True,\n",
    "                title=\"Average Readability Scores Radar Chart\"\n",
    "            )\n",
    "            st.plotly_chart(fig_radar, use_container_width=True)\n",
    "            \n",
    "            st.markdown(\"#### Line Chart: Readability Scores Across Samples\")\n",
    "            metric_selected_line = st.selectbox(\"Select a metric for line chart\", base_metrics, key=\"line_metric\")\n",
    "            line_df = df_scores.copy()\n",
    "            line_df[\"Sample\"] = range(1, len(line_df)+1)\n",
    "            plt.figure(figsize=(10,6))\n",
    "            sns.lineplot(data=line_df, x=\"Sample\", y=metric_selected_line, hue=\"Type\", marker=\"o\")\n",
    "            plt.title(f\"{metric_selected_line} Across Samples\", color='teal')\n",
    "            st.pyplot(plt)\n",
    "            \n",
    "            st.markdown(\"#### CDF Plot: Cumulative Distribution of Readability Scores\")\n",
    "            metric_selected_cdf = st.selectbox(\"Select a metric for CDF plot\", base_metrics, key=\"cdf_metric\")\n",
    "            plt.figure(figsize=(10,6))\n",
    "            for t in df_scores[\"Type\"].unique():\n",
    "                subset = df_scores[df_scores[\"Type\"]==t][metric_selected_cdf].dropna().sort_values()\n",
    "                cdf = np.linspace(0, 1, len(subset))\n",
    "                plt.plot(subset, cdf, label=t)\n",
    "            plt.title(f\"CDF of {metric_selected_cdf} Scores\", color='darkblue')\n",
    "            plt.xlabel(metric_selected_cdf)\n",
    "            plt.ylabel(\"CDF\")\n",
    "            plt.legend()\n",
    "            st.pyplot(plt)\n",
    "        \n",
    "        elif st.session_state.current_section == \"Download Results\":\n",
    "            st.markdown(\"#### Download Readability Scores\")\n",
    "            csv_scores = df_scores.to_csv(index=False)\n",
    "            st.download_button(\"Download Readability Scores CSV\", data=csv_scores, file_name=\"readability_scores.csv\", mime=\"text/csv\")\n",
    "            \n",
    "            st.markdown(\"#### Download T-test Results\")\n",
    "            csv_ttest = pd.DataFrame.from_dict(t_test_results, orient=\"index\").reset_index().rename(columns={\"index\": \"Metric\"}).to_csv(index=False)\n",
    "            st.download_button(\"Download T-test Results CSV\", data=csv_ttest, file_name=\"ttest_results.csv\", mime=\"text/csv\")\n",
    "else:\n",
    "    st.info(\"⚠️ Please upload or paste a text file to begin analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d7764-0594-4e93-aa63-900303560a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
